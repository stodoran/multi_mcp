# API Keys (configure at least one)
# OPENAI_API_KEY=sk-...
# GEMINI_API_KEY=...
# ANTHROPIC_API_KEY=...
# OPENROUTER_API_KEY=...

# Azure OpenAI (optional)
# Note: LiteLLM uses these specific variable names
# AZURE_API_KEY=...
# AZURE_API_BASE=https://...openai.azure.com/
# AZURE_API_VERSION=2025-04-01-preview  # Defaults to 2025-04-01-preview if not set

# AWS Bedrock (optional)
# Note: LiteLLM uses these specific variable names
# AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE
# AWS_SECRET_ACCESS_KEY=wJalrXutnxEaI/K7MDENG/bPxRfiCYEXAMPLEKEY
# AWS_REGION_NAME=us-east-1

# Model defaults
# Check names in config/models.yaml
# Cheap and fast
DEFAULT_MODEL=gpt-5-mini
DEFAULT_MODEL_LIST=mini,flash,haiku
DEFAULT_TEMPERATURE=0.2

# Powerful
# DEFAULT_MODEL=gpt-5.1
# DEFAULT_MODEL_LIST=gpt-5.1,gemini-3,sonnet
# DEFAULT_TEMPERATURE=0.2

# Server settings
# MAX_RETRIES=3
# MODEL_TIMEOUT_SECONDS=300.0

# File processing limits
# MAX_FILES_PER_REVIEW=100
# MAX_FILE_SIZE_KB=50

# Logging
# LOG_LEVEL=INFO

# Artifact logging (optional)
# Directory for saving LLM response artifacts
# Can be relative to base_path (e.g., "tmp/") or absolute (e.g., "/var/log/artifacts")
# Leave empty to disable artifact logging
ARTIFACTS_DIR=tmp/

